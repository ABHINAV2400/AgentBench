#!/usr/bin/env python3
"""
Demo script showing how real benchmark datasets work vs expected.json files
"""

def demo_human_eval():
    """Demo HumanEval with real test execution."""
    print("ğŸ§® HumanEval Demo - Real Test Execution")
    print("="*50)
    
    # Load real HumanEval problem
    from scenarios.human_eval.dataset import get_human_eval_problems, execute_test
    
    problems = get_human_eval_problems()
    problem = problems[0]  # First problem
    
    print(f"ğŸ“‹ Problem: {problem['task_id']}")
    print(f"ğŸ¯ Task: Extract decimal part of a number")
    print(f"ğŸ“ Prompt:\n{problem['prompt']}")
    
    # Test with correct solution
    print("\nâœ… Testing CORRECT solution:")
    correct_code = problem['prompt'] + problem['canonical_solution'] 
    result = execute_test(correct_code, problem['test'])
    print(f"Result: {result}")
    
    # Test with incorrect solution  
    print("\nâŒ Testing INCORRECT solution:")
    incorrect_solution = "    return number - int(number)  # Wrong approach"
    incorrect_code = problem['prompt'] + incorrect_solution
    result = execute_test(incorrect_code, problem['test'])
    print(f"Result: {result}")

def demo_swe_bench():
    """Demo SWE-bench with real patch evaluation.""" 
    print("\n\nğŸ’» SWE-bench Demo - Real Patch Evaluation")
    print("="*50)
    
    from scenarios.swe_bench.dataset import get_swe_bench_problems, evaluate_patch
    
    problems = get_swe_bench_problems()
    problem = problems[0]  # First problem
    
    print(f"ğŸ“‹ Problem: {problem['instance_id']}")
    print(f"ğŸ¯ Issue: {problem['problem_statement']}")
    print(f"ğŸ“¦ Repo: {problem['repo']}")
    
    # Test with correct patch
    print("\nâœ… Testing CORRECT patch:")
    correct_patch = problem['patch']
    result = evaluate_patch(problem['instance_id'], correct_patch)
    print(f"Similarity: {result['patch_similarity']:.2f}")
    print(f"Passed: {result['passed']}")
    
    # Test with incorrect patch
    print("\nâŒ Testing INCORRECT patch:")
    incorrect_patch = "--- a/some_file.py\n+++ b/some_file.py\n@@ -1,1 +1,1 @@\n-old_code\n+wrong_fix"
    result = evaluate_patch(problem['instance_id'], incorrect_patch)
    print(f"Similarity: {result['patch_similarity']:.2f}")
    print(f"Passed: {result['passed']}")

def compare_approaches():
    """Compare real benchmarks vs expected.json approach."""
    print("\n\nğŸ” Real Benchmarks vs Expected.json Comparison")
    print("="*60)
    
    print("ğŸ“Š REAL BENCHMARKS (What we now have):")
    print("âœ… Uses actual test cases from HumanEval dataset")
    print("âœ… Executes code against real test functions")
    print("âœ… Provides accurate pass/fail results")
    print("âœ… Matches research paper methodologies")
    print("âœ… No manual expected.json maintenance")
    print("âœ… Standardized evaluation metrics")
    
    print("\nğŸ“ EXPECTED.JSON APPROACH (What we replaced):")
    print("âŒ Manual creation of 'expected' results")
    print("âŒ Subjective evaluation criteria")
    print("âŒ No real test execution")
    print("âŒ Maintenance overhead for each scenario")
    print("âŒ Less accurate evaluation")
    print("âŒ Doesn't match research standards")

if __name__ == "__main__":
    print("ğŸš€ Comprehensive Agentic AI Benchmark Suite")
    print("Demo: Real Benchmark Datasets vs Expected.json")
    print("="*80)
    
    try:
        demo_human_eval()
        demo_swe_bench() 
        compare_approaches()
        
        print(f"\n\nğŸ¯ CONCLUSION:")
        print("Real benchmark datasets provide:")
        print("â€¢ Accurate, standardized evaluation")
        print("â€¢ Actual test execution and validation") 
        print("â€¢ Research-grade methodology")
        print("â€¢ Automatic scoring without manual expected.json files")
        print("\nThis is why established benchmarks don't need expected.json! ğŸ‰")
        
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        print("Make sure you're in the benchmark directory and have the required dependencies.")
    except Exception as e:
        print(f"âŒ Error: {e}")