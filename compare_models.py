#!/usr/bin/env python3
"""
AI Model Comparison Runner

Runs benchmarks across multiple models and generates comparison reports
"""

import argparse
from batch_runner import run_multiple_benchmarks
from visualize_results import save_visualizations
import time

def compare_all_models(scenarios=None, models=None):
    """Run comprehensive comparison across all models"""
    
    # Default models and scenarios
    if scenarios is None:
        scenarios = ['customer_support']
    
    if models is None:
        models = ['incredible_api', 'openai_gpt4']
    
    print("ğŸš€ Starting Comprehensive Model Comparison")
    print(f"ğŸ“Š Testing {len(models)} models on {len(scenarios)} scenarios")
    print(f"ğŸ¤– Models: {', '.join(models)}")
    print(f"ğŸ­ Scenarios: {', '.join(scenarios)}")
    print("="*60)
    
    # Record start time
    start_time = time.time()
    
    # Run all benchmarks
    run_multiple_benchmarks(scenarios, models)
    
    # Calculate total time
    total_time = time.time() - start_time
    
    print("\n" + "="*60)
    print("ğŸ¯ All benchmarks completed!")
    print(f"â±ï¸ Total execution time: {total_time:.2f} seconds")
    print("ğŸ“ˆ Generating visualizations...")
    
    # Generate visualizations
    save_visualizations()
    
    print("\nâœ… Model comparison complete!")
    print("ğŸ“Š Check the 'visualizations/' directory for results")
    print("ğŸŒ Open 'visualizations/benchmark_comparison.html' for interactive dashboard")

def main():
    parser = argparse.ArgumentParser(description='Compare AI models across benchmarks')
    parser.add_argument('--scenarios', nargs='+', default=['customer_support'],
                       help='Scenarios to test')
    parser.add_argument('--models', nargs='+', default=['incredible_api', 'openai_gpt4'],
                       help='Models to compare')
    
    args = parser.parse_args()
    
    compare_all_models(args.scenarios, args.models)

if __name__ == "__main__":
    main()